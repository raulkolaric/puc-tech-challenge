{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7814c617-a1a2-4e8a-b68d-6b66e3c8ad2d",
   "metadata": {},
   "source": [
    "# Detecção de Fraudes em Transações de Cartão de Crédito\n",
    "\n",
    "Este notebook implementa um pipeline de Machine Learning de ponta a ponta para detectar transações fraudulentas de cartão de crédito.\n",
    "Ele inclui:\n",
    "1.  Carregamento de dados (reais ou geração de dados sintéticos).\n",
    "2.  Engenharia de features para melhorar a capacidade preditiva do modelo.\n",
    "3.  Técnicas para lidar com datasets desbalanceados (SMOTE).\n",
    "4.  Divisão dos dados em conjuntos de treino e teste.\n",
    "5.  Treinamento de um modelo `RandomForestClassifier`.\n",
    "6.  Avaliação do modelo utilizando diversas métricas e análise da matriz de confusão.\n",
    "7.  Visualização da importância das features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec58084-8c7d-42ad-8b71-2c4901605788",
   "metadata": {},
   "source": [
    "## 1. Importações de Bibliotecas\n",
    "\n",
    "Este bloco de código Python é dedicado à **importação das bibliotecas e módulos necessários** para o projeto. Ele carrega `pandas` e `numpy` para manipulação de dados, `matplotlib` e `seaborn` para visualização, e diversos componentes do `scikit-learn` para geração de dados sintéticos (`make_classification`), divisão de dados (`train_test_split`), modelagem (`RandomForestClassifier`) e avaliação (`classification_report`, `confusion_matrix`, `accuracy_score`). Crucialmente, tenta importar `SMOTE` da biblioteca `imbalanced-learn`, definindo `IMBLEARN_AVAILABLE` para indicar se o oversampling com SMOTE estará funcional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713ec54-13bb-4c4e-94c9-a6fd9a1c2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de dados e matemática\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Visualização \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn: geração de dados, divisão, modelo, métricas\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Imbalanced-learn: SMOTE para oversampling\n",
    "# Certifique-se de ter instalado: pip install imbalanced-learn matplotlib seaborn\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "    print(\"Biblioteca 'imbalanced-learn' (para SMOTE) carregada com sucesso.\")\n",
    "except ImportError:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "    print(\"AVISO: Biblioteca 'imbalanced-learn' não encontrada. A funcionalidade SMOTE não estará disponível.\")\n",
    "    print(\"Para instalá-la, execute no seu terminal (com o venv ativo): pip install imbalanced-learn\")\n",
    "\n",
    "# Configurações de estilo para plots (opcional)\n",
    "# %matplotlib inline # Se estiver usando ambiente Jupyter clássico ou quiser garantir plots inline\n",
    "# plt.style.use('seaborn-v0_8-whitegrid') # Estilo de plot\n",
    "# sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Bibliotecas e módulos básicos importados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b67c2c-c47a-4bb8-a2a3-0f2942612440",
   "metadata": {},
   "source": [
    "## 2. Configuração do Experimento\n",
    "\n",
    "Este bloco Python define parâmetros cruciais para um experimento de machine learning. Ele permite escolher entre dados reais (`creditcard.csv`) ou sintéticos, habilitar/desabilitar a técnica de oversampling SMOTE no treino, e configurar hiperparâmetros para um modelo `RandomForestClassifier` (como `n_estimators`, `max_depth`). Além disso, estabelece um limiar de classificação (`0.5`) para converter probabilidades em classes e uma semente aleatória (`42`) para reprodutibilidade. Ao final, imprime as configurações selecionadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae7070-0a58-43c8-87a9-217eb4cd55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURAÇÕES DO EXPERIMENTO ---\n",
    "\n",
    "# Escolha da Fonte de Dados\n",
    "USAR_DADOS_REAIS = True  # Mude para False para usar dados sintéticos\n",
    "\n",
    "CAMINHO_DADOS_REAIS = 'creditcard.csv'\n",
    "COLUNA_ALVO_REAL = 'Class'\n",
    "\n",
    "# Configurações para Dados Sintéticos (se USAR_DADOS_REAIS = False)\n",
    "N_AMOSTRAS_SINTETICOS = 20000\n",
    "N_FEATURES_SINTETICOS = 20 # Número de features antes da engenharia\n",
    "PESOS_CLASSES_SINTETICOS = [0.99, 0.01] # Simula desbalanceamento (1% classe positiva)\n",
    "\n",
    "# Configuração do SMOTE (aplicado ao conjunto de treino)\n",
    "APLICAR_SMOTE_NO_TREINO = True # Mude para False para não aplicar SMOTE\n",
    "\n",
    "# Parâmetros do Modelo RandomForestClassifier\n",
    "# Estes são os parâmetros que você pode querer ajustar baseados nos seus testes com ui.py ou GridSearchCV\n",
    "MODEL_PARAMS = {\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': 30,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 5,\n",
    "    'random_state': 42,\n",
    "    'verbose': 0,          # Mantenha 0 para menos output no notebook, 1 ou mais para progresso do RF\n",
    "    'n_jobs': -1           # Usar todos os cores da CPU\n",
    "}\n",
    "\n",
    "# Limiar de Classificação para converter probabilidades em predições de classe\n",
    "LIMIAR_CLASSIFICACAO = 0.5\n",
    "\n",
    "# Semente aleatória para reprodutibilidade geral\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Configurações do experimento definidas.\")\n",
    "if USAR_DADOS_REAIS:\n",
    "    print(f\"  Modo: Dados Reais ('{CAMINHO_DADOS_REAIS}')\")\n",
    "else:\n",
    "    print(\"  Modo: Dados Sintéticos\")\n",
    "print(f\"  Aplicar SMOTE no treino: {'Sim' if APLICAR_SMOTE_NO_TREINO else 'Não'}\")\n",
    "print(f\"  Parâmetros do Modelo: {MODEL_PARAMS}\")\n",
    "print(f\"  Limiar de Classificação: {LIMIAR_CLASSIFICACAO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd45641-9912-4de9-8ec3-009af873c390",
   "metadata": {},
   "source": [
    "## 3. Funções Utilitárias para Dados\n",
    "\n",
    "Este bloco de código define **quatro funções Python essenciais** para um pipeline de machine learning. A função `load_csv_data` carrega dados de um arquivo CSV e separa features (X) do alvo (y). `generate_synthetic_data_scratch` cria dados sintéticos para classificação binária usando `make_classification`. `engineer_features_from_data` gera novas features a partir de um DataFrame existente (ex: `Amount_per_Time`, `Log1p_Amount`, features cíclicas de tempo). Por fim, `augment_data_smote` aplica a técnica SMOTE para lidar com desbalanceamento de classes nos dados de treino, se a biblioteca `imblearn` estiver disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b98e35-6ca3-490a-b38c-0e5342a2bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, target_column_name):\n",
    "    \"\"\"\n",
    "    Carrega dados de um arquivo CSV especificado e separa as features (X) e o alvo (y).\n",
    "    \"\"\"\n",
    "    print(f\"\\nTentando carregar dados de: {file_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dados carregados com sucesso de {file_path}.\")\n",
    "        if target_column_name in df.columns:\n",
    "            X = df.drop(target_column_name, axis=1)\n",
    "            y = df[target_column_name]\n",
    "            print(f\"Features (X) e alvo (y: '{target_column_name}') separados.\")\n",
    "            print(f\"Shape das Features (X): {X.shape}, Shape do Alvo (y): {y.shape}\")\n",
    "            return X, y\n",
    "        else:\n",
    "            print(f\"Erro: Coluna alvo '{target_column_name}' não encontrada em {file_path}.\")\n",
    "            print(f\"Colunas disponíveis: {df.columns.tolist()}\")\n",
    "            return None, None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo não encontrado em {file_path}. Verifique se o caminho está correto.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao carregar ou processar os dados: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def generate_synthetic_data_scratch(n_samples, n_features, class_weights, target_column_name, random_state):\n",
    "    \"\"\"\n",
    "    Gera um conjunto de dados sintético para classificação binária a partir do zero.\n",
    "    \"\"\"\n",
    "    print(\"\\nGerando dados sintéticos do zero...\")\n",
    "    # Ajuste n_informative e n_redundant para serem sempre válidos em relação a n_features\n",
    "    n_informative_actual = max(1, min(n_features -1, int(n_features * 0.75))) \n",
    "    if n_features <= n_informative_actual : # Make sure n_features is greater than n_informative\n",
    "        n_informative_actual = max(1, n_features-1) if n_features > 1 else 1\n",
    "\n",
    "\n",
    "    n_redundant_actual = max(0, n_features - n_informative_actual - 1)\n",
    "    if n_features == 1 and n_informative_actual == 1: # Edge case for single feature\n",
    "        n_redundant_actual = 0\n",
    "\n",
    "\n",
    "    X_synth, y_synth = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_informative_actual,\n",
    "        n_redundant=n_redundant_actual,\n",
    "        n_repeated=0,\n",
    "        n_classes=2,\n",
    "        n_clusters_per_class=1,\n",
    "        weights=class_weights,\n",
    "        flip_y=0.01,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    feature_names = [f'synthetic_feature_{i+1}' for i in range(X_synth.shape[1])]\n",
    "    X_df = pd.DataFrame(X_synth, columns=feature_names)\n",
    "    y_series = pd.Series(y_synth, name=target_column_name)\n",
    "    print(f\"Dados sintéticos gerados com shape X: {X_df.shape}, shape y: {y_series.shape}\")\n",
    "    class_dist = y_series.value_counts(normalize=True) * 100\n",
    "    print(f\"Distribuição das classes: \\nClasse 0: {class_dist.get(0, 0):.2f}%\\nClasse 1: {class_dist.get(1, 0):.2f}%\")\n",
    "    return X_df, y_series\n",
    "\n",
    "def engineer_features_from_data(X_input_df):\n",
    "    \"\"\"\n",
    "    Cria novas features a partir de um DataFrame de features existente.\n",
    "    \"\"\"\n",
    "    if X_input_df is None:\n",
    "        print(\"DataFrame de entrada para engenharia de features é None. Pulando.\")\n",
    "        return None\n",
    "    print(\"\\nRealizando engenharia de novas features a partir dos dados de entrada...\")\n",
    "    X_engineered = X_input_df.copy()\n",
    "    if 'Time' in X_engineered.columns and 'Amount' in X_engineered.columns:\n",
    "        X_engineered['Amount_per_Time'] = X_engineered['Amount'] / (X_engineered['Time'] + 1e-6)\n",
    "        print(\"  Feature criada: 'Amount_per_Time'\")\n",
    "    if 'Amount' in X_engineered.columns:\n",
    "        X_engineered['Log1p_Amount'] = np.log1p(X_engineered['Amount'])\n",
    "        print(\"  Feature criada: 'Log1p_Amount'\")\n",
    "    if 'V1' in X_engineered.columns and 'V2' in X_engineered.columns: # Example\n",
    "        X_engineered['V1_x_V2'] = X_engineered['V1'] * X_engineered['V2']\n",
    "        print(\"  Feature criada: 'V1_x_V2'\")\n",
    "    if 'Time' in X_engineered.columns:\n",
    "        print(\"  Criando features cíclicas de tempo (Hora do Dia)...\")\n",
    "        seconds_in_day = 24 * 60 * 60\n",
    "        X_engineered['HourOfDay'] = (X_engineered['Time'] % seconds_in_day) / 3600.0\n",
    "        X_engineered['HourOfDay_sin'] = np.sin(2 * np.pi * X_engineered['HourOfDay'] / 24.0)\n",
    "        X_engineered['HourOfDay_cos'] = np.cos(2 * np.pi * X_engineered['HourOfDay'] / 24.0)\n",
    "        X_engineered = X_engineered.drop('HourOfDay', axis=1)\n",
    "        print(\"  Features criadas: 'HourOfDay_sin', 'HourOfDay_cos'\")\n",
    "    print(f\"Engenharia de features completa. Novo shape X: {X_engineered.shape}\")\n",
    "    return X_engineered\n",
    "\n",
    "def augment_data_smote(X_input_df, y_input_series, random_state):\n",
    "    \"\"\"\n",
    "    Aumenta os dados usando SMOTE para lidar com o desbalanceamento de classes.\n",
    "    \"\"\"\n",
    "    if not IMBLEARN_AVAILABLE:\n",
    "        print(\"SMOTE requer imbalanced-learn. Retornando dados originais.\")\n",
    "        return X_input_df, y_input_series\n",
    "    if X_input_df is None or y_input_series is None:\n",
    "        print(\"X ou y de entrada para SMOTE é None. Retornando dados originais.\")\n",
    "        return X_input_df, y_input_series\n",
    "    print(\"\\nTentando aumentar os dados usando SMOTE...\")\n",
    "    try:\n",
    "        print(\"Distribuição de classes original no treino:\\n\", y_input_series.value_counts(normalize=True) * 100)\n",
    "        if len(y_input_series.value_counts()) < 2 or y_input_series.value_counts().min() < 2: # SMOTE needs min samples\n",
    "            print(\"Não há amostras suficientes na classe minoritária ou apenas uma classe presente. Pulando SMOTE.\")\n",
    "            return X_input_df, y_input_series\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_smote, y_smote = smote.fit_resample(X_input_df, y_input_series)\n",
    "        X_smote_df = pd.DataFrame(X_smote, columns=X_input_df.columns)\n",
    "        y_smote_series = pd.Series(y_smote, name=y_input_series.name)\n",
    "        print(\"SMOTE aplicado com sucesso ao conjunto de treino.\")\n",
    "        print(f\"Shape X após SMOTE: {X_smote_df.shape}, shape y após SMOTE: {y_smote_series.shape}\")\n",
    "        print(\"Nova distribuição de classes após SMOTE:\\n\", y_smote_series.value_counts(normalize=True) * 100)\n",
    "        return X_smote_df, y_smote_series\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante o SMOTE: {e}. Retornando dados originais.\")\n",
    "        return X_input_df, y_input_series\n",
    "\n",
    "print(\"Funções utilitárias de dados definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941e289-c1f4-49d6-836f-838c12e5d16f",
   "metadata": {},
   "source": [
    "## 4. Execução do Pipeline Principal\n",
    "\n",
    "Agora vamos executar o pipeline passo a passo, utilizando as configurações e funções definidas acima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a013c48-b612-45f8-8f45-df609e7f634b",
   "metadata": {},
   "source": [
    "### 4.1 Carregamento de Dados e Engenharia de Features\n",
    "\n",
    "Este bloco de código é responsável por **carregar ou gerar o conjunto de dados principal** (`X_final`, `y_final`). Se a configuração `USAR_DADOS_REAIS` for `True`, ele utiliza a função `load_csv_data` para carregar dados reais e, em seguida, aplica `engineer_features_from_data` para criar novas features. Caso contrário (dados sintéticos), ele chama `generate_synthetic_data_scratch`. Ao final, se os dados foram obtidos com sucesso, exibe informações como as dimensões, a distribuição da variável alvo e as primeiras linhas do DataFrame `X_final`, preparando-o para as próximas etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d80fa7-00d7-4664-a483-bcea8eecc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final, y_final = None, None\n",
    "\n",
    "if USAR_DADOS_REAIS:\n",
    "    X_initial, y_initial = load_csv_data(file_path=CAMINHO_DADOS_REAIS, target_column_name=COLUNA_ALVO_REAL)\n",
    "    if X_initial is not None:\n",
    "        X_final = engineer_features_from_data(X_initial)\n",
    "        y_final = y_initial # y não muda com a engenharia de features de X\n",
    "else:\n",
    "    X_final, y_final = generate_synthetic_data_scratch(\n",
    "        n_samples=N_AMOSTRAS_SINTETICOS,\n",
    "        n_features=N_FEATURES_SINTETICOS,\n",
    "        class_weights=PESOS_CLASSES_SINTETICOS,\n",
    "        target_column_name=COLUNA_ALVO_REAL, # Usando o mesmo nome de alvo para consistência\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(\"(Para dados sintéticos, a etapa de 'engineer_features_from_data' é opcional e dependeria dos nomes das features geradas)\")\n",
    "\n",
    "if X_final is not None and y_final is not None:\n",
    "    print(\"\\nDados prontos para a próxima etapa.\")\n",
    "    print(f\"Shape de X_final: {X_final.shape}\")\n",
    "    print(f\"Distribuição de y_final:\\n{y_final.value_counts(normalize=True)}\")\n",
    "    print(\"\\nPrimeiras 5 linhas de X_final:\")\n",
    "    display(X_final.head()) # Use display() para melhor formatação de DataFrames em Jupyter\n",
    "else:\n",
    "    print(\"ERRO: Falha no carregamento ou geração de dados. Não é possível continuar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff993ec2-4ad5-4722-bdff-65163781061c",
   "metadata": {},
   "source": [
    "### 4.2 Divisão em Treino e Teste\n",
    "\n",
    "Este trecho de código é responsável por **dividir o conjunto de dados final** (`X_final`, `y_final`) em subconjuntos de **treinamento e teste**. Primeiramente, ele verifica se os dados finais existem. Se sim, utiliza a função `train_test_split` para separar 25% dos dados para teste (`test_size=0.25`), mantendo a proporção das classes (`stratify=y_final`) e usando uma semente aleatória (`RANDOM_STATE`) para reprodutibilidade. Finalmente, exibe as dimensões e a distribuição da variável alvo nos conjuntos resultantes ou uma mensagem de erro/aviso se a divisão não puder ocorrer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea59fc6-8ad8-4816-930d-e47008a013d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "if X_final is not None and y_final is not None:\n",
    "    print(\"\\nDividindo os dados em conjuntos de treino e teste...\")\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_final, y_final,\n",
    "            test_size=0.25,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=y_final\n",
    "        )\n",
    "        print(\"Dados divididos com sucesso.\")\n",
    "        print(f\"  Shape de X_train: {X_train.shape}, Shape de y_train: {y_train.shape}\")\n",
    "        print(f\"  Shape de X_test: {X_test.shape}, Shape de y_test: {y_test.shape}\")\n",
    "        print(f\"  Distribuição do alvo no treino original (y_train):\\n{y_train.value_counts(normalize=True)}\")\n",
    "        print(f\"  Distribuição do alvo no teste (y_test):\\n{y_test.value_counts(normalize=True)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao dividir os dados: {e}\")\n",
    "else:\n",
    "    print(\"Dados finais (X_final, y_final) não estão disponíveis para divisão.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d7b73-1681-4916-862b-c33470690ad1",
   "metadata": {},
   "source": [
    "### 4.3 Aplicação de SMOTE (Opcional, no Conjunto de Treino)\n",
    "\n",
    "Este bloco de código **prepara os dados de treino** (`X_train`, `y_train`) para o modelo, com a **opção de aplicar a técnica SMOTE**. Inicialmente, `X_train_processed` e `y_train_processed` são criados como cópias dos dados de treino. Se a variável `APLICAR_SMOTE_NO_TREINO` for `True` e os dados de treino existirem, a função `augment_data_smote` é chamada para balancear as classes. Caso contrário, ou se SMOTE não for aplicado, os dados de treino originais são mantidos, com mensagens informativas sobre a ação tomada e a distribuição das classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6bfb50-75e6-44ae-ba65-5a1911294c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = X_train.copy() if X_train is not None else None\n",
    "y_train_processed = y_train.copy() if y_train is not None else None\n",
    "\n",
    "if X_train is not None and y_train is not None:\n",
    "    if APLICAR_SMOTE_NO_TREINO:\n",
    "        print(\"\\nAplicando SMOTE apenas ao conjunto de treino...\")\n",
    "        X_train_processed, y_train_processed = augment_data_smote(X_train, y_train, random_state=RANDOM_STATE)\n",
    "        # A função augment_data_smote já imprime os shapes e distribuições\n",
    "    else:\n",
    "        print(\"\\nSMOTE não aplicado ao conjunto de treino.\")\n",
    "        if X_train_processed is not None: # Apenas para printar se os dados existem\n",
    "             print(f\"Usando dados de treino originais: X_train_processed shape: {X_train_processed.shape}\")\n",
    "             print(f\"Distribuição de y_train_processed:\\n{y_train_processed.value_counts(normalize=True)}\")\n",
    "else:\n",
    "    print(\"Conjunto de treino não disponível. Pulando SMOTE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3361677-3822-42ad-8c15-e87985266aaa",
   "metadata": {},
   "source": [
    "### 4.4 Treinamento do Modelo\n",
    "\n",
    "Este bloco de código foca no **treinamento do modelo `RandomForestClassifier`**. Antes de treinar, ele ajusta os parâmetros do modelo (`MODEL_PARAMS`): se SMOTE não foi aplicado e `class_weight` não está definido, ele adiciona `class_weight='balanced'`; se SMOTE foi aplicado e `class_weight` é 'balanced', ele o remove para evitar redundância. Em seguida, instancia o `RandomForestClassifier` com os parâmetros ajustados e o treina usando os dados `X_train_processed` e `y_train_processed`, cronometrando a duração do processo. Tratamento de erros e mensagens de progresso/sucesso são incluídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37b4b2-15a1-4515-ba38-c4155d8acb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "training_duration = 0\n",
    "\n",
    "if X_train_processed is not None and y_train_processed is not None:\n",
    "    current_model_params_for_training = MODEL_PARAMS.copy()\n",
    "    if not APLICAR_SMOTE_NO_TREINO and 'class_weight' not in current_model_params_for_training:\n",
    "        current_model_params_for_training['class_weight'] = 'balanced'\n",
    "        print(\"Usando class_weight='balanced' no modelo (SMOTE não aplicado ao treino e não especificado nos params).\")\n",
    "    elif APLICAR_SMOTE_NO_TREINO and current_model_params_for_training.get('class_weight') == 'balanced':\n",
    "        print(\"Aviso: SMOTE foi aplicado, class_weight='balanced' pode ser redundante. Removendo class_weight.\")\n",
    "        if 'class_weight' in current_model_params_for_training:\n",
    "            del current_model_params_for_training['class_weight']\n",
    "            \n",
    "    model = RandomForestClassifier(**current_model_params_for_training)\n",
    "\n",
    "    print(f\"\\nTreinando RandomForestClassifier com parâmetros: {current_model_params_for_training}...\")\n",
    "    if current_model_params_for_training.get('verbose', 0) > 0:\n",
    "        print(\"(Scikit-learn 'verbose' mostrará o progresso da construção das árvores abaixo)\")\n",
    "    \n",
    "    start_training_time = time.time()\n",
    "    try:\n",
    "        model.fit(X_train_processed, y_train_processed)\n",
    "        end_training_time = time.time()\n",
    "        training_duration = end_training_time - start_training_time\n",
    "        print(f\"Modelo treinado com sucesso em {training_duration:.2f} segundos.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante o treinamento do modelo: {e}\")\n",
    "        model = None \n",
    "else:\n",
    "    print(\"Dados de treino processados não disponíveis. Pulando treinamento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c30d5-1eef-43e3-8bdd-762e0c465bd2",
   "metadata": {},
   "source": [
    "### 4.5 Avaliação do Modelo\n",
    "\n",
    "Este trecho de código **avalia o desempenho do modelo treinado** no conjunto de teste (`X_test`, `y_test`). Primeiramente, verifica se o modelo e os dados de teste estão disponíveis. Se sim, ele gera previsões de probabilidade, converte-as em classes usando o `LIMIAR_CLASSIFICACAO`, e então exibe um **relatório de classificação** detalhado e uma **matriz de confusão** visual (usando `seaborn` e `matplotlib`). Adicionalmente, calcula e imprime os componentes da matriz de confusão (VN, FP, FN, VP) e a **acurácia geral**, tratando possíveis erros durante o processo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ad1f1-62ef-47e8-9025-4b0af76e1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and X_test is not None and y_test is not None:\n",
    "    print(\"\\nAvaliando o modelo no conjunto de teste...\")\n",
    "    try:\n",
    "        proba_predictions = model.predict_proba(X_test)[:, 1]\n",
    "        print(f\"Utilizando limiar de classificação: {LIMIAR_CLASSIFICACAO}\")\n",
    "        predictions = (proba_predictions >= LIMIAR_CLASSIFICACAO).astype(int)\n",
    "\n",
    "        print(\"\\n--- Relatório de Classificação ---\")\n",
    "        print(classification_report(y_test, predictions, zero_division=0))\n",
    "\n",
    "        print(\"\\n--- Matriz de Confusão ---\")\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        \n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Não Fraude (Prev)', 'Fraude (Prev)'], \n",
    "                    yticklabels=['Não Fraude (Real)', 'Fraude (Real)'],\n",
    "                    annot_kws={\"size\": 12})\n",
    "        plt.title('Matriz de Confusão', fontsize=14)\n",
    "        plt.ylabel('Classe Real', fontsize=12)\n",
    "        plt.xlabel('Classe Prevista', fontsize=12)\n",
    "        plt.show()\n",
    "        \n",
    "        tn, fp, fn, tp = 0,0,0,0 \n",
    "        if cm.size == 4: \n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.size == 1 and len(np.unique(y_test)) == 1 : \n",
    "             if y_test.iloc[0] == 0: tn = cm[0,0]\n",
    "             else: tp = cm[0,0]\n",
    "        \n",
    "        print(f\"\\nDetalhes da Matriz de Confusão:\")\n",
    "        print(f\"Verdadeiros Negativos (Não-Fraudes OK): {tn}\")\n",
    "        print(f\"Falsos Positivos (Não-Fraudes -> Fraude): {fp} <-- Erro Tipo I\")\n",
    "        print(f\"Falsos Negativos (Fraudes -> Não Fraude): {fn} <-- Erro Tipo II (CRÍTICO para fraude)\")\n",
    "        print(f\"Verdadeiros Positivos (Fraudes OK): {tp}\")\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f\"\\nAcurácia Geral: {accuracy:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante a avaliação ou predição: {e}\")\n",
    "else:\n",
    "    print(\"Modelo não treinado ou dados de teste não disponíveis. Pulando avaliação.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8a2d1-8d4a-427a-8440-1cc235d9cd2e",
   "metadata": {},
   "source": [
    "### 4.6 Importância das Features (Random Forest)\n",
    "\n",
    "Este bloco de código visualiza a **importância das features** conforme determinado pelo modelo treinado (se ele suportar, como o RandomForest). Ele verifica se o modelo existe, se `X_final` é um DataFrame e se o atributo `feature_importances_` está disponível. Em caso afirmativo, extrai as importâncias, associa-as aos nomes das colunas de `X_final`, e exibe as **15 features mais importantes** em uma tabela e em um gráfico de barras horizontal (`seaborn`), facilitando a interpretação de quais variáveis mais influenciaram as previsões do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f001475-b7e6-4fc2-9731-99fc19e3c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and isinstance(X_final, pd.DataFrame) and hasattr(model, 'feature_importances_'):\n",
    "    print(\"\\n--- Importância das Features (Top 15) ---\")\n",
    "    try:\n",
    "        importances = model.feature_importances_\n",
    "        feature_names = X_final.columns \n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        print(\"Primeiras 15 features mais importantes:\")\n",
    "        display(feature_importance_df.head(15))\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15), \n",
    "                    palette='viridis', hue='feature', legend=False) # UPDATED LINE\n",
    "        plt.title('Importância das Features (Top 15)', fontsize=14)\n",
    "        plt.xlabel('Importância', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.tight_layout() \n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao calcular/plotar importância das features: {e}\")\n",
    "else:\n",
    "    print(\"Modelo não treinado, X_final não é DataFrame ou o modelo não suporta 'feature_importances_'. Pulando.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edb528-70f6-433b-bc94-612aaee7bcd6",
   "metadata": {},
   "source": [
    "## 5. Conclusão e Próximos Passos\n",
    "Ao longo deste projeto, meu objetivo foi desenvolver um modelo de Machine Learning eficaz para a detecção de fraudes em transações de cartão de crédito, utilizando um dataset realista e altamente desbalanceado.\n",
    "\n",
    "Performance do Modelo e Descobertas:\n",
    "\n",
    "Com a configuração atual, que envolveu o uso de dados reais (creditcard.csv), engenharia de features e a aplicação da técnica SMOTE no conjunto de treino para lidar com o desbalanceamento de classes, consegui resultados bastante promissores com o modelo RandomForestClassifier. Nos meus melhores experimentos, utilizando parâmetros como n_estimators=150, max_depth=30, min_samples_leaf=1 e min_samples_split=5 (ou 2), e um limiar de classificação de 0.5, alcancei um F1-score para a classe de fraude (Classe 1) em torno de 0.87.\n",
    "\n",
    "Impacto do SMOTE: A aplicação do SMOTE no conjunto de treino foi crucial. Antes de sua utilização (ou quando apenas o class_weight='balanced' era usado no modelo), a revocação (recall) para a classe de fraude era significativamente mais baixa. Após o SMOTE, que balanceou artificialmente as classes no conjunto de treino, observei um aumento substancial na revocação – o modelo passou a identificar uma porcentagem muito maior das fraudes reais (por exemplo, de um recall inicial baixo para a faixa de 0.80-0.84). Isso, no entanto, veio com uma ligeira queda na precisão em alguns casos, indicando um aumento nos falsos positivos, o que é um trade-off comum.\n",
    "\n",
    "Efeito da Engenharia de Features: As features que criei, como Amount_per_Time, Log1p_Amount e as interações cíclicas da feature Time (HourOfDay_sin, HourOfDay_cos), contribuíram para a performance do modelo. A análise de importância das features (feature importances) mostrou que algumas dessas novas features, juntamente com certas features V do dataset original (como V14, V4, V10, V12, V17), foram bastante influentes nas decisões do modelo.\n",
    "\n",
    "Efeito do Limiar de Classificação: Mantive o limiar de classificação em 0.5 para a maioria dos experimentos reportados. No entanto, reconheço que ajustar este limiar (usando a saída de predict_proba()) é uma próxima etapa importante para otimizar o equilíbrio entre precisão e revocação, dependendo dos custos associados a falsos positivos versus falsos negativos no contexto de negócio.\n",
    "Considerando o F1-score de 0.87 (com Precisão de 0.90 e Revocação de 0.84 para a classe de fraude), considero que o modelo atingiu um bom nível de performance, identificando 103 das 123 fraudes no conjunto de teste, com apenas 11 alarmes falsos.\n",
    "\n",
    "Próximos Passos e Melhorias Futuras:\n",
    "\n",
    "Apesar dos resultados encorajadores, há várias avenidas para exploração e melhoria:\n",
    "\n",
    "Busca de Hiperparâmetros Mais Exaustiva: Utilizar GridSearchCV ou RandomizedSearchCV com uma grade de parâmetros mais ampla para o RandomForestClassifier (e potencialmente para o SMOTE) poderia refinar ainda mais o modelo. O GridSearchCV que executei com um número limitado de candidatos já confirmou um bom conjunto de parâmetros, mas uma busca mais extensa pode revelar combinações ainda melhores.\n",
    "Testar Outros Algoritmos: Explorar algoritmos de Gradient Boosting como XGBoost, LightGBM ou CatBoost, que são conhecidos por sua alta performance em datasets tabulares e desbalanceados, e oferecem parâmetros específicos para lidar com o desbalanceamento (ex: scale_pos_weight).\n",
    "\n",
    "Engenharia de Features Avançada: Dedicar mais tempo à análise exploratória dos dados para identificar outras interações ou transformações de features que possam ser mais discriminativas.\n",
    "Análise Detalhada de Erros: Investigar os casos de Falsos Positivos e Falsos Negativos para entender se há padrões nesses erros que possam guiar melhorias no pré-processamento ou na escolha do modelo.\n",
    "Técnicas de Detecção de Anomalias: Como a fraude é um evento raro, abordagens de detecção de anomalias (como Isolation Forest ou One-Class SVM) poderiam ser exploradas como alternativas ou em conjunto com os modelos de classificação.\n",
    "\n",
    "Limiar de Classificação Otimizado: Realizar uma análise sistemática do impacto de diferentes limiares de classificação na curva Precision-Recall para escolher um limiar que otimize o F1-score ou atenda a um requisito específico de negócio (por exemplo, um recall mínimo desejado).\n",
    "Considerar Custos de Classificação Incorreta: Em um cenário real, atribuir custos diferentes para Falsos Positivos e Falsos Negativos e otimizar o modelo ou o limiar com base nesses custos seria uma etapa importante.\n",
    "\n",
    "Em conclusão, este projeto me proporcionou uma experiência prática valiosa na construção de um pipeline de detecção de fraudes, desde a preparação dos dados até a avaliação do modelo, e me deu uma base sólida para futuras explorações e otimizações neste domínio desafiador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffadeeca-3d75-4f35-b475-c9a79f2da447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
